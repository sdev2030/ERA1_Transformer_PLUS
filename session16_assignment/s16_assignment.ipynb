{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d503e7-866d-4bb2-8a18-647623e78509",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext tensorboard\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a661307b-8162-4f51-ab4b-7caceeacaa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sn/anaconda3/envs/fastai2022/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from ssl import SSLSession\n",
    "from tokenize import Whitespace\n",
    "# from model_transformer import build_transformer\n",
    "from dataset import BilingualDataset, causal_mask\n",
    "from config import get_config, get_weights_file_path\n",
    "\n",
    "import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "# Launch TensorBoard SSLSession\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26eb19b-a1aa-4e19-91df-053823487b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from model_transformer import transformerModel, saveCallback #, PrintCallback #YOLOv3\n",
    "from train import greedy_decode, get_ds\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pytorch_lightning import LightningModule, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0fd738-3837-4e93-8c1a-c47f42fb8bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "AVAIL_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12ab24b-ffa8-4841-805f-fbe8ef0b8aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length of source sentence: 471\n",
      "Max Length of target sentence: 482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelSummary, RichProgressBar\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "num_examples = 2\n",
    "\n",
    "cfg = get_config()\n",
    "cfg['batch_size'] = 8\n",
    "cfg['preload'] = None\n",
    "cfg['num_epochs'] = 10\n",
    "\n",
    "# Make sure the weights folder exists\n",
    "Path(cfg['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(cfg)\n",
    "\n",
    "#Tensorboard\n",
    "writer = SummaryWriter(cfg['experiment_name'])\n",
    "\n",
    "model = transformerModel(cfg, tokenizer_src, tokenizer_tgt, writer, num_examples)\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[ModelSummary(max_depth=-1), lr_monitor], #saveCallback()], # PrintCallback()], RichProgressBar(leave=True)\n",
    "#     default_root_dir=\"/home/sn/ERAv1/checkpoints/\",\n",
    "    enable_checkpointing=True,\n",
    "    precision=16,\n",
    "    devices=AVAIL_GPUS, \n",
    "    max_epochs=cfg['num_epochs'],\n",
    "    # limit_train_batches = 10,\n",
    "    limit_val_batches = num_examples,\n",
    "    log_every_n_steps=500\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9ebf8a-dbca-47f1-85a7-c1a57e4bb062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "    | Name                                                  | Type                    | Params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0   | model                                                 | Transformer             | 68.1 M\n",
      "1   | model.encoder                                         | Encoder                 | 9.4 M \n",
      "2   | model.encoder.layers                                  | ModuleList              | 9.4 M \n",
      "3   | model.encoder.layers.0                                | EncoderBlock            | 3.1 M \n",
      "4   | model.encoder.layers.0.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "5   | model.encoder.layers.0.self_attention_block.w_q       | Linear                  | 262 K \n",
      "6   | model.encoder.layers.0.self_attention_block.w_k       | Linear                  | 262 K \n",
      "7   | model.encoder.layers.0.self_attention_block.w_v       | Linear                  | 262 K \n",
      "8   | model.encoder.layers.0.self_attention_block.w_o       | Linear                  | 262 K \n",
      "9   | model.encoder.layers.0.self_attention_block.dropout   | Dropout                 | 0     \n",
      "10  | model.encoder.layers.0.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "11  | model.encoder.layers.0.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "12  | model.encoder.layers.0.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "13  | model.encoder.layers.0.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "14  | model.encoder.layers.0.residual_connections           | ModuleList              | 4     \n",
      "15  | model.encoder.layers.0.residual_connections.0         | ResidualConnection      | 2     \n",
      "16  | model.encoder.layers.0.residual_connections.0.dropout | Dropout                 | 0     \n",
      "17  | model.encoder.layers.0.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "18  | model.encoder.layers.0.residual_connections.1         | ResidualConnection      | 2     \n",
      "19  | model.encoder.layers.0.residual_connections.1.dropout | Dropout                 | 0     \n",
      "20  | model.encoder.layers.0.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "21  | model.encoder.layers.1                                | EncoderBlock            | 3.1 M \n",
      "22  | model.encoder.layers.1.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "23  | model.encoder.layers.1.self_attention_block.w_q       | Linear                  | 262 K \n",
      "24  | model.encoder.layers.1.self_attention_block.w_k       | Linear                  | 262 K \n",
      "25  | model.encoder.layers.1.self_attention_block.w_v       | Linear                  | 262 K \n",
      "26  | model.encoder.layers.1.self_attention_block.w_o       | Linear                  | 262 K \n",
      "27  | model.encoder.layers.1.self_attention_block.dropout   | Dropout                 | 0     \n",
      "28  | model.encoder.layers.1.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "29  | model.encoder.layers.1.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "30  | model.encoder.layers.1.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "31  | model.encoder.layers.1.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "32  | model.encoder.layers.1.residual_connections           | ModuleList              | 4     \n",
      "33  | model.encoder.layers.1.residual_connections.0         | ResidualConnection      | 2     \n",
      "34  | model.encoder.layers.1.residual_connections.0.dropout | Dropout                 | 0     \n",
      "35  | model.encoder.layers.1.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "36  | model.encoder.layers.1.residual_connections.1         | ResidualConnection      | 2     \n",
      "37  | model.encoder.layers.1.residual_connections.1.dropout | Dropout                 | 0     \n",
      "38  | model.encoder.layers.1.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "39  | model.encoder.layers.2                                | EncoderBlock            | 3.1 M \n",
      "40  | model.encoder.layers.2.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "41  | model.encoder.layers.2.self_attention_block.w_q       | Linear                  | 262 K \n",
      "42  | model.encoder.layers.2.self_attention_block.w_k       | Linear                  | 262 K \n",
      "43  | model.encoder.layers.2.self_attention_block.w_v       | Linear                  | 262 K \n",
      "44  | model.encoder.layers.2.self_attention_block.w_o       | Linear                  | 262 K \n",
      "45  | model.encoder.layers.2.self_attention_block.dropout   | Dropout                 | 0     \n",
      "46  | model.encoder.layers.2.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "47  | model.encoder.layers.2.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "48  | model.encoder.layers.2.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "49  | model.encoder.layers.2.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "50  | model.encoder.layers.2.residual_connections           | ModuleList              | 4     \n",
      "51  | model.encoder.layers.2.residual_connections.0         | ResidualConnection      | 2     \n",
      "52  | model.encoder.layers.2.residual_connections.0.dropout | Dropout                 | 0     \n",
      "53  | model.encoder.layers.2.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "54  | model.encoder.layers.2.residual_connections.1         | ResidualConnection      | 2     \n",
      "55  | model.encoder.layers.2.residual_connections.1.dropout | Dropout                 | 0     \n",
      "56  | model.encoder.layers.2.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "57  | model.encoder.norm                                    | LayerNormalization      | 2     \n",
      "58  | model.decoder                                         | Decoder                 | 12.6 M\n",
      "59  | model.decoder.layers                                  | ModuleList              | 12.6 M\n",
      "60  | model.decoder.layers.0                                | DecoderBlock            | 4.2 M \n",
      "61  | model.decoder.layers.0.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "62  | model.decoder.layers.0.self_attention_block.w_q       | Linear                  | 262 K \n",
      "63  | model.decoder.layers.0.self_attention_block.w_k       | Linear                  | 262 K \n",
      "64  | model.decoder.layers.0.self_attention_block.w_v       | Linear                  | 262 K \n",
      "65  | model.decoder.layers.0.self_attention_block.w_o       | Linear                  | 262 K \n",
      "66  | model.decoder.layers.0.self_attention_block.dropout   | Dropout                 | 0     \n",
      "67  | model.decoder.layers.0.cross_attentioin_block         | MultiHeadAttentionBlock | 1.0 M \n",
      "68  | model.decoder.layers.0.cross_attentioin_block.w_q     | Linear                  | 262 K \n",
      "69  | model.decoder.layers.0.cross_attentioin_block.w_k     | Linear                  | 262 K \n",
      "70  | model.decoder.layers.0.cross_attentioin_block.w_v     | Linear                  | 262 K \n",
      "71  | model.decoder.layers.0.cross_attentioin_block.w_o     | Linear                  | 262 K \n",
      "72  | model.decoder.layers.0.cross_attentioin_block.dropout | Dropout                 | 0     \n",
      "73  | model.decoder.layers.0.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "74  | model.decoder.layers.0.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "75  | model.decoder.layers.0.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "76  | model.decoder.layers.0.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "77  | model.decoder.layers.0.residual_connections           | ModuleList              | 6     \n",
      "78  | model.decoder.layers.0.residual_connections.0         | ResidualConnection      | 2     \n",
      "79  | model.decoder.layers.0.residual_connections.0.dropout | Dropout                 | 0     \n",
      "80  | model.decoder.layers.0.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "81  | model.decoder.layers.0.residual_connections.1         | ResidualConnection      | 2     \n",
      "82  | model.decoder.layers.0.residual_connections.1.dropout | Dropout                 | 0     \n",
      "83  | model.decoder.layers.0.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "84  | model.decoder.layers.0.residual_connections.2         | ResidualConnection      | 2     \n",
      "85  | model.decoder.layers.0.residual_connections.2.dropout | Dropout                 | 0     \n",
      "86  | model.decoder.layers.0.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "87  | model.decoder.layers.1                                | DecoderBlock            | 4.2 M \n",
      "88  | model.decoder.layers.1.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "89  | model.decoder.layers.1.self_attention_block.w_q       | Linear                  | 262 K \n",
      "90  | model.decoder.layers.1.self_attention_block.w_k       | Linear                  | 262 K \n",
      "91  | model.decoder.layers.1.self_attention_block.w_v       | Linear                  | 262 K \n",
      "92  | model.decoder.layers.1.self_attention_block.w_o       | Linear                  | 262 K \n",
      "93  | model.decoder.layers.1.self_attention_block.dropout   | Dropout                 | 0     \n",
      "94  | model.decoder.layers.1.cross_attentioin_block         | MultiHeadAttentionBlock | 1.0 M \n",
      "95  | model.decoder.layers.1.cross_attentioin_block.w_q     | Linear                  | 262 K \n",
      "96  | model.decoder.layers.1.cross_attentioin_block.w_k     | Linear                  | 262 K \n",
      "97  | model.decoder.layers.1.cross_attentioin_block.w_v     | Linear                  | 262 K \n",
      "98  | model.decoder.layers.1.cross_attentioin_block.w_o     | Linear                  | 262 K \n",
      "99  | model.decoder.layers.1.cross_attentioin_block.dropout | Dropout                 | 0     \n",
      "100 | model.decoder.layers.1.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "101 | model.decoder.layers.1.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "102 | model.decoder.layers.1.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "103 | model.decoder.layers.1.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "104 | model.decoder.layers.1.residual_connections           | ModuleList              | 6     \n",
      "105 | model.decoder.layers.1.residual_connections.0         | ResidualConnection      | 2     \n",
      "106 | model.decoder.layers.1.residual_connections.0.dropout | Dropout                 | 0     \n",
      "107 | model.decoder.layers.1.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "108 | model.decoder.layers.1.residual_connections.1         | ResidualConnection      | 2     \n",
      "109 | model.decoder.layers.1.residual_connections.1.dropout | Dropout                 | 0     \n",
      "110 | model.decoder.layers.1.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "111 | model.decoder.layers.1.residual_connections.2         | ResidualConnection      | 2     \n",
      "112 | model.decoder.layers.1.residual_connections.2.dropout | Dropout                 | 0     \n",
      "113 | model.decoder.layers.1.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "114 | model.decoder.layers.2                                | DecoderBlock            | 4.2 M \n",
      "115 | model.decoder.layers.2.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "116 | model.decoder.layers.2.self_attention_block.w_q       | Linear                  | 262 K \n",
      "117 | model.decoder.layers.2.self_attention_block.w_k       | Linear                  | 262 K \n",
      "118 | model.decoder.layers.2.self_attention_block.w_v       | Linear                  | 262 K \n",
      "119 | model.decoder.layers.2.self_attention_block.w_o       | Linear                  | 262 K \n",
      "120 | model.decoder.layers.2.self_attention_block.dropout   | Dropout                 | 0     \n",
      "121 | model.decoder.layers.2.cross_attentioin_block         | MultiHeadAttentionBlock | 1.0 M \n",
      "122 | model.decoder.layers.2.cross_attentioin_block.w_q     | Linear                  | 262 K \n",
      "123 | model.decoder.layers.2.cross_attentioin_block.w_k     | Linear                  | 262 K \n",
      "124 | model.decoder.layers.2.cross_attentioin_block.w_v     | Linear                  | 262 K \n",
      "125 | model.decoder.layers.2.cross_attentioin_block.w_o     | Linear                  | 262 K \n",
      "126 | model.decoder.layers.2.cross_attentioin_block.dropout | Dropout                 | 0     \n",
      "127 | model.decoder.layers.2.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "128 | model.decoder.layers.2.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "129 | model.decoder.layers.2.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "130 | model.decoder.layers.2.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "131 | model.decoder.layers.2.residual_connections           | ModuleList              | 6     \n",
      "132 | model.decoder.layers.2.residual_connections.0         | ResidualConnection      | 2     \n",
      "133 | model.decoder.layers.2.residual_connections.0.dropout | Dropout                 | 0     \n",
      "134 | model.decoder.layers.2.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "135 | model.decoder.layers.2.residual_connections.1         | ResidualConnection      | 2     \n",
      "136 | model.decoder.layers.2.residual_connections.1.dropout | Dropout                 | 0     \n",
      "137 | model.decoder.layers.2.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "138 | model.decoder.layers.2.residual_connections.2         | ResidualConnection      | 2     \n",
      "139 | model.decoder.layers.2.residual_connections.2.dropout | Dropout                 | 0     \n",
      "140 | model.decoder.layers.2.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "141 | model.decoder.norm                                    | LayerNormalization      | 2     \n",
      "142 | model.src_embed                                       | InputEmbeddings         | 15.4 M\n",
      "143 | model.src_embed.embedding                             | Embedding               | 15.4 M\n",
      "144 | model.tgt_embed                                       | InputEmbeddings         | 15.4 M\n",
      "145 | model.tgt_embed.embedding                             | Embedding               | 15.4 M\n",
      "146 | model.src_pos                                         | PositionalEncoding      | 0     \n",
      "147 | model.src_pos.dropout                                 | Dropout                 | 0     \n",
      "148 | model.tgt_pos                                         | PositionalEncoding      | 0     \n",
      "149 | model.tgt_pos.dropout                                 | Dropout                 | 0     \n",
      "150 | model.projection_layer                                | ProjectionLayer         | 15.4 M\n",
      "151 | model.projection_layer.proj                           | Linear                  | 15.4 M\n",
      "152 | loss_fn                                               | CrossEntropyLoss        | 0     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "68.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "68.1 M    Total params\n",
      "272.582   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Then d’Artagnan ceased knocking, and prayed with an accent so full of anxiety and promises, terror and cajolery, that his voice was of a nature to reassure the most fearful.\n",
      "    TARGET: Alors d'Artagnan cessa de frapper et pria, avec un accent si plein d'inquiétude et de promesses, d'effroi et de cajolerie, que sa voix était de nature à rassurer de plus peureux.\n",
      " PREDICTED: Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Le but de la duchesse, que toutefois elle ne s’avouait pas, était de presser le mariage du marquis Crescenzi : Fabrice, de son côté, fit la route dans des transports de bonheur fous, et qui semblèrent ridicules à sa tante.\n",
      "    TARGET: The Duchessa's object, which however she did not admit to herself, was to hasten the Marchese Crescenzi's marriage; Fabrizio, for his part, spent the journey in wild transports of joy, which seemed to his aunt absurd.\n",
      " PREDICTED: Cours Cours Cours incliné incliné incliné incliné incliné incliné incliné incliné past past past past past past past past past past past past past vieillissant vieillissant Cours Cours Cours Cours Cours Cours Cours Cours Cours Cours mémorable mémorable mémorable mémorable vieillissant vieillissant vieillissant vieillissant vieillissant vieillissant vieillissant vieillissant vieillissant\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaacf8bba9cb419aa4266051d0d6d4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Je vais monter sur votre cheval et galoper jusqu’à une lieue au-delà de Côme ; je vais à Milan me jeter aux genoux du vice-roi.\n",
      "    TARGET: I am going to mount your horse and gallop a league beyond Como; I am going to Milan to throw myself at the Viceroy's feet.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: With lowered faces, and swinging one leg crossed over the other knee, they uttered deep sighs at intervals; each one was inordinately bored, and yet none would be the first to go.\n",
      "    TARGET: La figure basse et le jarret sur le genou, ils dandinaient leur jambe, tout en poussant par intervalles un gros soupir; et chacun s’ennuyait d’une façon démesurée; c’était pourtant à qui ne partirait pas.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Taking you all in all, I see a spark in you which must not be neglected.\n",
      "    TARGET: Au total, je vois en vous une étincelle qu’il ne faut pas négliger.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: (Cette chimère était ridicule : le prince avait beaucoup d’esprit, mais, à force d’y rêver, il en était devenu amoureux fou.)\n",
      "    TARGET: (This was an absurd fantasy: the Prince had abundance of brains, but, by dint of dreaming of it, he had fallen madly in love with the idea. )\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"No, not before.\n",
      "    TARGET: --Non, pas avant, c'est déjà très tôt!\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: When he reappeared: 'Heaven knows, you damned idler,' his father said to him, 'whether you will ever have enough honour to pay me for the cost of your keep, which I have been advancing to you all these years!\n",
      "    TARGET: Quand il reparut : – Dieu sait, maudit paresseux, lui dit son père, si tu auras jamais assez d’honneur pour me payer le prix de ta nourriture, que j’avance depuis tant d’années !\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But really, and upon my honour, I will try to do what I think to be the wisest; and now I hope you are satisfied.\"\n",
      "    TARGET: En toute sincérité, j’essaierai de faire ce qui me semblera le plus raisonnable. Et maintenant, j’espere que vous voila satisfaite.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I cannot but wonder, however, at her having any such fears now, because, if he had at all cared about me, we must have met, long ago.\n",
      "    TARGET: Il sait certainement que je suis a Londres ; une phrase de Caroline me l’a laissé a entendre.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: My grandfather used often to tell us that in his young days he had had a tutor.\n",
      "    TARGET: Mon grand-père nous racontait souvent que, dans sa jeunesse, il avait eu un précepteur.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Faut-il dire que ce courrier était porteur de tous les moyens d’argent et de tous les passeports nécessaires ?\n",
      "    TARGET: Need it be said that this courier was the bearer of all the means of obtaining money and all the necessary passports?\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But now we have to prove the connection between the man and the beast.\n",
      "    TARGET: Il nous reste à prouver la relation entre l’homme et le chien.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: While he was endeavouring to make Julien understand what he felt, a slight sound made them turn their heads.\n",
      "    TARGET: Pendant qu’il s’efforçait de faire comprendre ce sentiment par Julien, un bruit léger leur fit tourner la tête.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Au début, ce pouvoir vague et terrifiant ne s’étendait que sur les révoltés qui apres avoir embrassé la religion des Mormons voulaient ensuite soit la dénaturer, soit l’abandonner.\n",
      "    TARGET: At first this vague and terrible power was exercised only upon the recalcitrants who, having embraced the Mormon faith, wished afterwards to pervert or to abandon it.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But what a servant he was, clever, zealous, indefatigable, not indiscreet, not talkative, and he might have been with reason proposed as a model for all his biped brothers in the Old and New Worlds!\n",
      "    TARGET: Mais quel domestique, adroit, zélé, infatigable, pas indiscret, pas bavard, et on eût pu avec raison le proposer pour modèle à tous ses confrères bipèdes de l'ancien et du nouveau monde!\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Show yourself precisely as you were a week before you were honoured with her favours.'\n",
      "    TARGET: Montrez-vous précisément tel que vous étiez huit jours avant d’être honoré de ses bontés.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And he was only one of many, for all day when they did not come in masses they came in twos and threes with as brave a face as if the whole army were at their heels.\n",
      "    TARGET: Et il y en eut bien d'autres comme lui, car pendant toute la journée, quand ils n'arrivaient pas en masses, ils venaient par deux, par trois, l'air aussi résolu que s'ils avaient toute l'armée sur leurs talons.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Ayrton!\n",
      "    TARGET: -- Ayrton!\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"That is strong,\" she said, when she had finished: \"I relish it.\"\n",
      "    TARGET: «C'est fort, dit-elle, lorsqu'elle eut fini; j'aime cela.»\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The king took it with a trembling hand, looked for the address, which was wanting, became very pale, opened it slowly, then seeing by the first words that it was addressed to the King of Spain, he read it rapidly.\n",
      "    TARGET: Le roi la prit d'une main tremblante, chercha l'adresse, qui manquait, devint très pâle, l'ouvrit lentement, puis, voyant par les premiers mots qu'elle était adressée au roi d'Espagne, il lut très rapidement.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He pressed a metal button and at once the propeller slowed down significantly.\n",
      "    TARGET: Il pressa un bouton de métal, et aussitôt la vitesse de l'hélice fut très diminuée.\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "#             ckpt_path=\"/ERAv1/session15_assignment/weights/s15Model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1071b13f-39e0-4938-a2f8-e91a52a1cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d4f73b63a4cf02f7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d4f73b63a4cf02f7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5fd165-f637-4832-b0ca-5552bce66a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai2022)",
   "language": "python",
   "name": "fastai2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
